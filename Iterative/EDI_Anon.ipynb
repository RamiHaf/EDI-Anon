{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168be264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from shutil import copyfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, f1_score, auc, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from torchvision.utils import save_image\n",
    "from true_classify import *\n",
    "from Utils import *\n",
    "from anonymization_methods import *\n",
    "from datasets import *\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from collections import Counter\n",
    "import matplotlib.image as mpimg\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import xlrd\n",
    "import openpyxl\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d283082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to be used by pytorch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75addd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path of the data, the model, and the output. Also define the number the the classes in the dataset \n",
    "source_path = 'Path of the Data to be anonimyzed'\n",
    "\n",
    "\n",
    "class_names = [folder for folder in os.listdir(source_path) if os.path.isdir(os.path.join(source_path, folder))]\n",
    "num_classes = len(class_names)\n",
    "\n",
    "file_list = os.listdir(source_path)\n",
    "model_dir = 'Path of the trained FR model'\n",
    "\n",
    "output_path = 'Path to save the anonimyzed images'\n",
    "\n",
    "save_roc_dir = 'Path to save the ROC'\n",
    "\n",
    "excel_file_path = 'Path to save the excel file with the numberical results of accuracy and f1_score'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a64045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new transform with additional data augmentations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948e3817",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define the model and load the pretrained model\n",
    "\n",
    "model = models.convnext_base(pretrained=True)\n",
    "model.classifier[2]=nn.Linear(1024,num_classes)\n",
    "model.load_state_dict(torch.load('Path of the trained FR model.pt'))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d311191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hyperparameter to be used by the EDI_Anon to calculate the loss in order to anonimyze the images\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0c83a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the q which in 100-the percentage of the affect onn the images\n",
    "q=90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50893a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting Parameter values\n",
    "\n",
    "#For Simple Blur which is k in the paper\n",
    "blur_kernel_size = 40\n",
    "\n",
    "#For Simple Pixelate which is n in the paper\n",
    "pixel_size = 20\n",
    "\n",
    "\n",
    "\n",
    "#For DP Pix\n",
    "b = 32\n",
    "m = 16\n",
    "eps = 100\n",
    "\n",
    "#For DP blur\n",
    "b0 = 32 #recommended half of the original b\n",
    "k = 99\n",
    "sigma = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3cf21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the original accuracy and f1_score of the original data\n",
    "\n",
    "start_time = time.time()\n",
    "new_batch_size = 1\n",
    "new_test_path = source_path\n",
    "our_test_loader = create_test_loader(new_test_path, new_batch_size)\n",
    "final_acc, correct_examples, labels, logits = test_images_classification(model, device, our_test_loader, excel_file_path, save_roc_dir)\n",
    "prev_acc = final_acc\n",
    "print(source_path)\n",
    "print(prev_acc*100)\n",
    "\n",
    "end_time = time.time()\n",
    "acc_time = end_time - start_time\n",
    "print(f\"\\nAcc execution time: {acc_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e26732",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Setting high number of iteration to give the EDI_Anon the time it needs to effectivly anonimyze the dataset\n",
    "for itera in range(0,200000):\n",
    "    # Iterate through all correct examples\n",
    "    print('Starting the anonymization for the interation number '+str(itera)+' ------------->')\n",
    "    for i in tqdm(range(len(correct_examples))):\n",
    "        x, correct_label, prediction = correct_examples[i], labels[i], logits[i]\n",
    "        y = get_second_largest(logits[i])\n",
    "        y = torch.tensor([y]).to(device)\n",
    "\n",
    "        # Create the output directory for this iteration\n",
    "        iteration_out_path = f\"{output_path}/Iteration_{itera}/\"\n",
    "\n",
    "        class_label = labels[i]\n",
    "        class_subfolder = class_names[class_label]            \n",
    "\n",
    "        class_output_path = os.path.join(iteration_out_path, class_subfolder)\n",
    "        os.makedirs(class_output_path, exist_ok=True)\n",
    "\n",
    "        # Clone the original image and enable image gradient computation\n",
    "        annonymized_image = x.clone()\n",
    "        annonymized_image.requires_grad = True\n",
    "\n",
    "        # Calculate the loss based on the model, image, and criterion\n",
    "        output, loss = calculate_loss(model, annonymized_image, y, criterion)\n",
    "\n",
    "        # test if the image is correctly classified then we need to anonimyze it\n",
    "        if(output.item() == labels[i].item()):\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            img_grad = annonymized_image.grad.data\n",
    "\n",
    "            # select the anonimyzation function you want to use\n",
    "\n",
    "#             perturbed = create_pixelated_image(x, pixel_size)    \n",
    "#             perturbed = create_blurred_image(x, blur_kernel_size)      \n",
    "#             perturbed = create_dp_pixelated_image(x, b, m, eps)\n",
    "#             perturbed = create_dp_blurred_image(x, b0, m, eps, k, sigma)\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "            # Calculate noise based on the original and perturbed images\n",
    "            noise = calculate_noise(x, perturbed, device)\n",
    "\n",
    "            # Optimize the gradients using the quantile value (q)\n",
    "            optimized_gradients = optimize_gradients(img_grad, q)\n",
    "            optimized_gradients = optimized_gradients.to(device)\n",
    "            optimized_gradients = optimized_gradients.cpu().detach().numpy().squeeze()\n",
    "            optimized_gradients = np.transpose(optimized_gradients, (1, 2, 0))\n",
    "\n",
    "            # Update the noise using optimized gradients\n",
    "            updated_noise = noise * optimized_gradients\n",
    "\n",
    "\n",
    "            # Create the anonymized image by subtracting/adding the updated noise from the original image\n",
    "            numpy_image = x.cpu().detach().numpy().squeeze()\n",
    "            cv2_image = np.transpose(numpy_image, (1, 2, 0))\n",
    "            cv2_image = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            annonymized_image = cv2_image - updated_noise \n",
    "            \n",
    "            # unnormalizing the image to save it with the correct colors\n",
    "            annonymized_image = annonymized_image*255\n",
    "\n",
    "            \n",
    "            # Save the anonymized image to the output directory\n",
    "            save_image(annonymized_image, i, correct_label, class_output_path, pixel_size, 100-q)  \n",
    "\n",
    "        # If the model didn't correctly calssify the image then save it as it is\n",
    "        else:\n",
    "            numpy_image = x.cpu().detach().numpy().squeeze()\n",
    "            cv2_image = np.transpose(numpy_image, (1, 2, 0))\n",
    "            cv2_image = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB)\n",
    "            cv2_image = cv2.normalize(cv2_image, None, alpha = 0, beta = 255, norm_type = cv2.NORM_MINMAX, dtype = cv2.CV_32F)\n",
    "            save_image(cv2_image, i, correct_label, class_output_path, pixel_size, 100-q)\n",
    "\n",
    "\n",
    "    # test the model performance on the anonimyzed data at the end of each iteration\n",
    "    our_test_loader = create_test_loader(iteration_out_path, batch_size=1)\n",
    "    accuracy, correct_examples, labels, logits = test_images_classification(model, device, our_test_loader, excel_file_path, save_roc_dir)\n",
    "    print(f\"\\nAccuracy: {accuracy*100} %\")\n",
    "\n",
    "    # If we have achieved the requiered RIR, then end the EDI_Anon processes\n",
    "    if(accuracy*100 < 'set the RIR you want to achieve'):\n",
    "        break\n",
    "\n",
    "print(f\"All images processed for iteration numberÂ {itera}.\")\n",
    "end_time = time.time()\n",
    "Anon_execution_time = end_time - start_time\n",
    "print(f\"\\nAnon Time: {Anon_execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d24dde9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
